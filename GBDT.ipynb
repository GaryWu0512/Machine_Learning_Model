{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss(metaclass=ABCMeta):\n",
    "    '''\n",
    "    Absctract base class for loss function.\n",
    "    \n",
    "        pred() takes GBDT outputs, i.e., the \"score\" as its inputs, and returns predictions.\n",
    "        g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
    "        h() is the heassian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
    "\n",
    "    All inputs and outputs are numpy arrays.\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def pred(self,score):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def g(self,true,score):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def h(self,true,score):\n",
    "        pass\n",
    "\n",
    "#TODO: loss of least square regression and binary logistic regression\n",
    "class mse(loss):\n",
    "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
    "    def pred(self,score):\n",
    "        return score\n",
    "\n",
    "    def g(self,true,score):\n",
    "        return score-true\n",
    "\n",
    "    def h(self,true,score):\n",
    "        return np.ones_like(score)\n",
    "\n",
    "class log(loss):\n",
    "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
    "    def pred(self,score):\n",
    "        return 1.0/(1.0+np.exp(-score))\n",
    "\n",
    "    def g(self,true,score):\n",
    "        pred=self.pred(score)\n",
    "        return pred-true\n",
    "\n",
    "    def h(self,true,score):\n",
    "        pred=self.pred(score)\n",
    "        return pred*(1.0-pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of Random Forest\n",
    "class RF(object):\n",
    "    '''\n",
    "    Class of Random Forest\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth d_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        rf = 0.5, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.num_trees = num_trees\n",
    "        self.rf = rf\n",
    "\n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        \n",
    "        if self.loss == 'mse':\n",
    "            self.loss = mse()\n",
    "        if self.loss == 'log':\n",
    "            self.loss = log()\n",
    "        n = len(target)\n",
    "            \n",
    "        self.trees = []\n",
    "        self.score_start = target.astype('float').mean()\n",
    "        # score is the accumulated prediction $y^k$ and should be updated once a tree has been added\n",
    "        score = np.ones(len(train)) * self.score_start\n",
    "        for i in range(self.num_trees):\n",
    "            \n",
    "            # bootstrap sampling (random sampling with replacement)\n",
    "            index = np.random.choice(n, n)\n",
    "            \n",
    "            tree = Tree(n_threads = self.n_threads,\n",
    "                      max_depth = self.max_depth, min_sample_split = self.min_sample_split,\n",
    "                      lamda = self.lamda, gamma = self.gamma, rf = self.rf)\n",
    "            tree.fit(train[index], g = self.loss.g(target[index], score[index]), h = self.loss.h(target[index], score[index]))\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            print('tree', i)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        \n",
    "        score = np.ones(len(test)) * self.score_start\n",
    "        for i in range(self.num_trees):\n",
    "            score += (1.0/float(self.num_trees)) * self.trees[i].predict(test)\n",
    "        return self.loss.pred(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of GBDT\n",
    "class GBDT(object):\n",
    "    '''\n",
    "    Class of gradient boosting decision tree (GBDT)\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth D_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        learning_rate: The learning rate eta of GBDT.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        learning_rate = 0.1, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_trees = num_trees\n",
    "\n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        \n",
    "        if self.loss == 'mse':\n",
    "            self.loss = mse()\n",
    "        if self.loss == 'log':\n",
    "            self.loss = log()\n",
    "            \n",
    "        self.trees = []\n",
    "        self.score_start = target.astype('float').mean()\n",
    "        # score is the accumulated prediction $y^k$ and should be updated once a tree has been added\n",
    "        score = np.ones(len(train)) * self.score_start\n",
    "        for i in range(self.num_trees):\n",
    "            \n",
    "            tree = Tree(n_threads = self.n_threads,\n",
    "                      max_depth = self.max_depth, min_sample_split = self.min_sample_split,\n",
    "                      lamda = self.lamda, gamma = self.gamma)\n",
    "            tree.fit(train, g = self.loss.g(target, score), h = self.loss.h(target, score))\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            score += self.learning_rate * tree.predict(train)\n",
    "            print('tree', i)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        \n",
    "        score = np.ones(len(test)) * self.score_start\n",
    "        for i in range(self.num_trees):\n",
    "            score += self.learning_rate * self.trees[i].predict(test)\n",
    "        return self.loss.pred(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of a node on a tree\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    Data structure that are used for storing a node on a tree.\n",
    "    \n",
    "    A tree is presented by a set of nested TreeNodes,\n",
    "    with one TreeNode pointing two child TreeNodes,\n",
    "    until a tree leaf is reached.\n",
    "    \n",
    "    A node on a tree can be either a leaf node or a non-leaf node.\n",
    "\n",
    "    Parameters:\n",
    "        is_leaf: If is TreeNode is a leaf, i.e., node $j\\in L_k$ or $j\\in N_k$ in hw.\n",
    "        score: The prediction (score) of a tree leaf, the $w^k_j$ in hw.\n",
    "        split_feature: The split feature of a tree node, the $p_j$ in hw.\n",
    "        split_threshold: The split threshold of a tree node, the $\\tau_j$ in hw.\n",
    "        left_child: Pointing to a child TreeNode, \n",
    "                    where the value of split feature is less than the split threshold.\n",
    "        right_child: Pointing to a child TreeNode, \n",
    "                    where the value of split features is greater than or equal to the split threshold.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "        is_leaf = False, score = None,\n",
    "        split_feature = None, split_threshold = None,\n",
    "        left_child = None, right_child = None):\n",
    "        \n",
    "        self.is_leaf = is_leaf\n",
    "        self.score = score\n",
    "        self.split_feature = split_feature\n",
    "        self.split_threshold = split_threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of single tree\n",
    "class Tree(object):\n",
    "    '''\n",
    "    Class of a single decision tree in GBDT\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_threads = None, \n",
    "                 max_depth = 3, min_sample_split = 10,\n",
    "                 lamda = 1, gamma = 0, nb = False, rf = 0):\n",
    "        self.n_threads = n_threads\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.nb = nb\n",
    "        self.rf = rf\n",
    "\n",
    "    def fit(self, train, g, h):\n",
    "        '''\n",
    "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
    "        g and h are gradient and hessian respectively.\n",
    "        '''\n",
    "        self.n, self.m = train.shape\n",
    "        self.tree = self.construct_tree(train, g, h, self.max_depth)\n",
    "        return self\n",
    "\n",
    "    def predict(self,test):\n",
    "        '''\n",
    "        test is the test data matrix, and must be numpy arrays (an n_test x m matrix).\n",
    "        Return predictions (scores) as an array.\n",
    "        Multiprocessing is supported for prediction.\n",
    "        '''\n",
    "        pool = Pool(self.n_threads)\n",
    "        f = partial(self.predict_single, self.tree)\n",
    "        result = np.array(pool.map(f, test))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return result\n",
    "\n",
    "    def predict_single(self, treenode, test):\n",
    "        '''\n",
    "        The predict method for a single sample point.\n",
    "        test must be numpy array (a m-dim vector)\n",
    "        Return prediction (score) as a number.\n",
    "        '''\n",
    "        if treenode.is_leaf:\n",
    "            return treenode.score\n",
    "        else:\n",
    "            if test[treenode.split_feature] < treenode.split_threshold:\n",
    "                return self.predict_single(treenode.left_child, test)\n",
    "            else:\n",
    "                return self.predict_single(treenode.right_child, test)\n",
    "\n",
    "    def construct_tree(self, train, g, h, max_depth):\n",
    "        '''\n",
    "        Tree construction, which is recursively used to grow a tree.\n",
    "        First we should check if we should stop further splitting.\n",
    "        \n",
    "        The stopping conditions include:\n",
    "            1. tree reaches max_depth $d_{max}$\n",
    "            2. The number of sample points at current node is less than min_sample_split, i.e., $n_{min}$\n",
    "            3. gain <= 0\n",
    "            \n",
    "        The above stopping conditions indicate and cover two extra stopping conditions:\n",
    "            4. Targets take only one value.\n",
    "            5. Each feature takes only one value.\n",
    "            \n",
    "            By careful design, we could avoid checking condition 4 and 5 explicitly.\n",
    "            In function find_threshold(), the best_gain is set to 0 initially.\n",
    "            So if there are no further feature to split,\n",
    "            or all the targets take the same value,\n",
    "            the return value of best_gain would be zero.\n",
    "            Thus condition 3 would be satisfied,\n",
    "            and no further splitting would be done.\n",
    "            To conclude, we need only to check condition 1,2 and 3.\n",
    "        '''\n",
    "        # stopping condition 1 & 2\n",
    "        if max_depth == 0 or len(train) < self.min_sample_split:\n",
    "        # we start from max_depth, and reduce it by 1 everytime when we split a node, \n",
    "        # so max_depth == 0 means the tree reaches the maximum depth\n",
    "            return TreeNode(is_leaf = True, score = self.leaf_score(g, h))\n",
    "        \n",
    "        feature, threshold, gain = self.find_best_decision_rule(train, g, h)\n",
    "\n",
    "        # stopping condition 3\n",
    "        if gain <= self.gamma:\n",
    "        # the gain here used is different from hw in that it does not includes the last -位\n",
    "            return TreeNode(is_leaf = True, score = self.leaf_score(g, h))\n",
    "\n",
    "        index = train[:, feature] < threshold\n",
    "        left_child = self.construct_tree(train[index], g[index], h[index], max_depth-1)\n",
    "        right_child = self.construct_tree(train[~index], g[~index], h[~index], max_depth-1)\n",
    "        \n",
    "        return TreeNode(split_feature = feature, split_threshold = threshold, \n",
    "                        left_child = left_child, right_child = right_child)\n",
    "    \n",
    "    def leaf_score(self, g, h):\n",
    "        '''\n",
    "        Given the gradient and hessian of a tree leaf node,\n",
    "        return the optimal weight $w^k_j$ at this leaf, which is -G/(H+位).\n",
    "        '''\n",
    "        return -np.sum(g)/(np.sum(h)+self.lamda)\n",
    "\n",
    "    def leaf_loss(self, g, h):\n",
    "        '''\n",
    "        Given the gradient and hessian of a tree leaf node,\n",
    "        return the minimized loss at this leaf, which is -0.5*G^2/(H+位).\n",
    "        '''\n",
    "        return -0.5*np.square(np.sum(g))/(np.sum(h)+self.lamda)\n",
    "\n",
    "    def find_best_decision_rule(self, train, g, h):\n",
    "        '''\n",
    "        Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j, \n",
    "        train is the training data assigned to node j\n",
    "        g and h are the corresponding 1st and 2nd derivatives for each data point in train\n",
    "        g and h should be vectors of the same length as the number of data points in train\n",
    "        \n",
    "        for each feature, we find the best threshold by find_threshold(),\n",
    "        a [threshold, best_gain] list is returned for each feature.\n",
    "        Then we select the feature with the largest best_gain,\n",
    "        and return the best decision rule [feature, treshold] together with its gain.\n",
    "        '''\n",
    "        \n",
    "        # find best threshold per feature\n",
    "        pool = Pool(self.n_threads)\n",
    "        f = partial(self.find_threshold, g, h)\n",
    "        if self.rf > 0:\n",
    "            rand_subset = np.random.choice(self.m, int(self.rf * self.m), replace = False)\n",
    "            thresholds = np.array(pool.map(f, train.T[rand_subset]))\n",
    "        else:\n",
    "            thresholds = np.array(pool.map(f, train.T))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        # find the best feature with the largest gain\n",
    "        feature=np.argmax(thresholds[:, 1], axis=0)\n",
    "        threshold=thresholds[feature, 0]\n",
    "        gain=thresholds[feature, 1]\n",
    "        if self.rf > 0:\n",
    "            feature = rand_subset[feature]\n",
    "        \n",
    "        return feature, threshold, gain\n",
    "    \n",
    "    def find_threshold(self, g, h, train):\n",
    "        '''\n",
    "        Given a particular feature $p_j$,\n",
    "        return the best split threshold $\\tau_j$ together with the gain that is achieved.\n",
    "        '''\n",
    "        \n",
    "        # precompute 1st term of the gain\n",
    "        loss = self.leaf_loss(g, h)\n",
    "        \n",
    "        # initialization\n",
    "        threshold = None\n",
    "        best_gain = 0\n",
    "        \n",
    "        # avoid same feature values from different data points (samples)\n",
    "        unq=np.unique(train)\n",
    "        \n",
    "        # compare all the possible thresholds\n",
    "        for i in range(1, len(unq)):\n",
    "            \n",
    "            # try one threshold and compute gain\n",
    "            this_threshold = (unq[i-1] + unq[i])/2\n",
    "            \n",
    "            index = train < this_threshold\n",
    "            left_loss = self.leaf_loss(g[index], h[index])\n",
    "            right_loss = self.leaf_loss(g[~index], h[~index])\n",
    "            # first three terms of the gain (except the last -位)\n",
    "            this_gain = loss - left_loss - right_loss\n",
    "            \n",
    "            if this_gain > best_gain:\n",
    "                threshold = this_threshold\n",
    "                best_gain = this_gain\n",
    "                \n",
    "        return [threshold, best_gain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluation (you can use code from previous homeworks)\n",
    "\n",
    "# RMSE\n",
    "def root_mean_square_error(pred, y):\n",
    "    rmse = np.sqrt(np.mean((pred - y)**2))\n",
    "    return rmse\n",
    "\n",
    "# precision\n",
    "def accuracy(pred, y):\n",
    "    pred = pred > 0.5\n",
    "    return sum(np.equal(pred, y)) / float(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GBDT at 0x11685b8d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test GBDT\n",
    "model = GBDT(n_threads=8, loss='mse', \n",
    "             max_depth=3, min_sample_split = 10, \n",
    "             lamda = 1.0, gamma = 0.1,\n",
    "             learning_rate=0.1, num_trees=100)\n",
    "train = np.random.randn(1000, 50)\n",
    "target = np.random.randn(1000)\n",
    "model.fit(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.6856408799133467\n"
     ]
    }
   ],
   "source": [
    "print('RMSE =', root_mean_square_error(pred, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: GBDT regression on boston house price dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree 0\n",
      "tree 1\n",
      "tree 2\n",
      "tree 3\n",
      "tree 4\n",
      "tree 5\n",
      "tree 6\n",
      "tree 7\n",
      "tree 8\n",
      "tree 9\n",
      "tree 10\n",
      "tree 11\n",
      "tree 12\n",
      "tree 13\n",
      "tree 14\n",
      "tree 15\n",
      "tree 16\n",
      "tree 17\n",
      "tree 18\n",
      "tree 19\n",
      "tree 20\n",
      "tree 21\n",
      "tree 22\n",
      "tree 23\n",
      "tree 24\n",
      "tree 25\n",
      "tree 26\n",
      "tree 27\n",
      "tree 28\n",
      "tree 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GBDT at 0x113031438>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train GBDT regression on boston\n",
    "model = GBDT(n_threads=4, loss='mse', \n",
    "             max_depth=5, min_sample_split = 10, \n",
    "             lamda = 2.0, gamma = 0.1,\n",
    "             learning_rate=0.1, num_trees=30)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT Training RMSE = 1.616135667597893\n",
      "GBDT Test RMSE = 3.576038913706025\n"
     ]
    }
   ],
   "source": [
    "# evaluate GBDT regression on boston\n",
    "print('GBDT Training RMSE =', root_mean_square_error(model.predict(X_train), y_train))\n",
    "print('GBDT Test RMSE =', root_mean_square_error(model.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree 0\n",
      "tree 1\n",
      "tree 2\n",
      "tree 3\n",
      "tree 4\n",
      "tree 5\n",
      "tree 6\n",
      "tree 7\n",
      "tree 8\n",
      "tree 9\n",
      "tree 10\n",
      "tree 11\n",
      "tree 12\n",
      "tree 13\n",
      "tree 14\n",
      "tree 15\n",
      "tree 16\n",
      "tree 17\n",
      "tree 18\n"
     ]
    }
   ],
   "source": [
    "# train RF regression on boston\n",
    "model = RF(n_threads=4, loss='mse', \n",
    "             max_depth=5, min_sample_split = 10, \n",
    "             lamda = 2.0, gamma = 0.1,\n",
    "             rf = 0.5, num_trees=30)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Training RMSE = 3.5786261133089026\n",
      "RF Test RMSE = 4.101972771458844\n"
     ]
    }
   ],
   "source": [
    "# evaluate RF regression on boston\n",
    "print('RF Training RMSE =', root_mean_square_error(model.predict(X_train), y_train))\n",
    "print('RF Test RMSE =', root_mean_square_error(model.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 1 1 1 0 1] [0.86323301 0.57355438 0.64687948 0.87353876 0.76004527 0.58468808\n",
      " 0.91684898 0.83928138 0.53350157 0.5666469 ]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(y_test[:10], y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: GBDT classification on breast cancer dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree 0\n",
      "tree 1\n",
      "tree 2\n",
      "tree 3\n",
      "tree 4\n",
      "tree 5\n",
      "tree 6\n",
      "tree 7\n",
      "tree 8\n",
      "tree 9\n",
      "tree 10\n",
      "tree 11\n",
      "tree 12\n",
      "tree 13\n",
      "tree 14\n",
      "tree 15\n",
      "tree 16\n",
      "tree 17\n",
      "tree 18\n",
      "tree 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GBDT at 0x1a1c4126a0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train GBDT classification on breast_cancer\n",
    "model = GBDT(n_threads=4, loss='log', \n",
    "             max_depth=5, min_sample_split = 10, \n",
    "             lamda = 1.0, gamma = 0.1,\n",
    "             learning_rate=0.1, num_trees=20)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT Training accuracy = 0.9974874371859297\n",
      "GBDT Test accuracy = 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# evaluate GBDT classification on brest cancer\n",
    "print('GBDT Training accuracy =', accuracy(model.predict(X_train), y_train))\n",
    "print('GBDT Test accuracy =', accuracy(model.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree 0\n",
      "tree 1\n",
      "tree 2\n",
      "tree 3\n",
      "tree 4\n",
      "tree 5\n",
      "tree 6\n",
      "tree 7\n",
      "tree 8\n",
      "tree 9\n",
      "tree 10\n",
      "tree 11\n",
      "tree 12\n",
      "tree 13\n",
      "tree 14\n",
      "tree 15\n",
      "tree 16\n",
      "tree 17\n",
      "tree 18\n",
      "tree 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RF at 0x1a1c4cc080>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train RF classification on breast_cancer\n",
    "model = RF(n_threads=4, loss='log', \n",
    "             max_depth=5, min_sample_split = 10, \n",
    "             lamda = 1.0, gamma = 0.1,\n",
    "             rf = 0.3, num_trees=20)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Training accuracy = 0.9874371859296482\n",
      "RF Test accuracy = 0.9415204678362573\n"
     ]
    }
   ],
   "source": [
    "# evaluate RF classification on brest cancer\n",
    "print('RF Training accuracy =', accuracy(model.predict(X_train), y_train))\n",
    "print('RF Test accuracy =', accuracy(model.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: GBDT classification on credit-g dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/')\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree 0\n",
      "tree 1\n",
      "tree 2\n",
      "tree 3\n",
      "tree 4\n",
      "tree 5\n",
      "tree 6\n",
      "tree 7\n",
      "tree 8\n",
      "tree 9\n",
      "tree 10\n",
      "tree 11\n",
      "tree 12\n",
      "tree 13\n",
      "tree 14\n",
      "tree 15\n",
      "tree 16\n",
      "tree 17\n",
      "tree 18\n",
      "tree 19\n",
      "tree 20\n",
      "tree 21\n",
      "tree 22\n",
      "tree 23\n",
      "tree 24\n",
      "tree 25\n",
      "tree 26\n",
      "tree 27\n",
      "tree 28\n",
      "tree 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GBDT at 0x1a1c4ebd68>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train GBDT regression on credit-g\n",
    "model = GBDT(n_threads=4, loss='log', \n",
    "             max_depth=5, min_sample_split = 10, \n",
    "             lamda = 2.0, gamma = 0.2,\n",
    "             learning_rate=0.3, num_trees=30)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT Training accuracy = 0.9657142857142857\n",
      "GBDT Test accuracy = 0.7866666666666666\n"
     ]
    }
   ],
   "source": [
    "# evaluate GBDT classification on credit-g\n",
    "print('GBDT Training accuracy =', accuracy(model.predict(X_train), y_train))\n",
    "print('GBDT Test accuracy =', accuracy(model.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree 0\n",
      "tree 1\n",
      "tree 2\n",
      "tree 3\n",
      "tree 4\n",
      "tree 5\n",
      "tree 6\n",
      "tree 7\n",
      "tree 8\n",
      "tree 9\n",
      "tree 10\n",
      "tree 11\n",
      "tree 12\n",
      "tree 13\n",
      "tree 14\n",
      "tree 15\n",
      "tree 16\n",
      "tree 17\n",
      "tree 18\n",
      "tree 19\n",
      "tree 20\n",
      "tree 21\n",
      "tree 22\n",
      "tree 23\n",
      "tree 24\n",
      "tree 25\n",
      "tree 26\n",
      "tree 27\n",
      "tree 28\n",
      "tree 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RF at 0x1a1c559320>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train RF regression on credit-g\n",
    "model = RF(n_threads=4, loss='log', \n",
    "             max_depth=5, min_sample_split = 10, \n",
    "             lamda = 2.0, gamma = 0.2,\n",
    "             rf = 0.3, num_trees=30)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Training accuracy = 0.7957142857142857\n",
      "RF Test accuracy = 0.7633333333333333\n"
     ]
    }
   ],
   "source": [
    "# evaluate GBDT classification on credit-g\n",
    "print('RF Training accuracy =', accuracy(model.predict(X_train), y_train))\n",
    "print('RF Test accuracy =', accuracy(model.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
